# -*- coding: utf-8 -*-
"""Note Sentences

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Qg2aJV1E7Jd7_7TdUeFT_pU5acUXjNcT
"""

!pip uninstall -y transformers torch torchvision torchaudio

!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
!pip install transformers pdfplumber

import pdfplumber
import re
import json
from tqdm import tqdm
from transformers import pipeline
import torch
print("GPU disponible :", torch.cuda.is_available())

# -*- coding: utf-8 -*-
"""
Extraction du raisonnement juridique pour les oppositions de marques
"""

import pdfplumber
import re
import json
from tqdm import tqdm
from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering
import torch
import nltk

# Télécharger toutes les ressources NLTK nécessaires
print("Téléchargement des ressources NLTK...")
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')

from nltk.tokenize import sent_tokenize, word_tokenize

print("GPU disponible :", torch.cuda.is_available())

# Initialiser les modèles
print("\nChargement des modèles...")
classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli", device=0 if torch.cuda.is_available() else -1)
qa_pipeline = pipeline("question-answering", model="deepset/roberta-base-squad2", device=0 if torch.cuda.is_available() else -1)
print("✓ Modèles chargés")

# Mots-clés spécifiques pour guider l'extraction
similarity_keywords = {
    "visual_similarity": ["visually", "visual similarity", "visual differences", "appearance",
                         "graphic", "figurative", "stylized", "design", "logo", "device",
                         "element", "component", "colour", "color", "shape", "medium degree"],
    "aural_similarity": ["aurally", "phonetically", "pronunciation", "sounds", "syllables",
                        "spoken", "verbal", "phonetic similarity", "pronounced", "identical",
                        "articulated", "SEE-TECH", "heard"],
    "conceptual_similarity": ["conceptually", "meaning", "concept", "idea", "semantic",
                             "conceptual similarity", "evokes", "conveys", "understood",
                             "perception", "message", "significance"]
}

distinctiveness_keywords = {
    "inherent_distinctiveness": ["inherent distinctive", "distinctive character", "allusive",
                                "descriptive", "invented", "arbitrary", "suggestive",
                                "neither allusive nor descriptive", "no allusive qualities"],
    "enhanced_distinctiveness": ["enhanced through use", "reputation", "market share",
                                "evidence of use", "acquired distinctiveness", "use made of it"],
    "distinctiveness_conclusion": ["medium degree", "high degree", "low degree",
                                  "distinctive to", "level of distinctiveness",
                                  "medium and high degree", "between a medium"]
}

confusion_keywords = {
    "direct_confusion": ["direct confusion", "mistaking one", "mistake the marks",
                        "imperfect recollection", "same mark", "misremembered",
                        "overlooked", "encounter them"],
    "indirect_confusion": ["indirect confusion", "economic link", "brand extension",
                          "same undertaking", "related companies", "updated presentation",
                          "re-brand", "variant"],
    "global_assessment": ["global assessment", "average consumer", "overall impression",
                         "factors", "taking account", "all relevant factors", "borne in mind"],
    "interdependency": ["interdependency", "offset", "lesser degree", "greater degree",
                       "similarity between marks", "similarity between goods", "vice versa"],
    "final_conclusion": ["likelihood of confusion", "I find", "therefore", "conclusion",
                        "opposition succeeds", "opposition fails", "there is a likelihood"]
}

def pdf_to_text(pdf_path):
    """Extrait le texte d'un PDF"""
    with pdfplumber.open(pdf_path) as pdf:
        return "\n".join([page.extract_text() for page in pdf.pages if page.extract_text()])

def split_into_chunks(text, max_length=1500, overlap=200):
    """Divise le texte en chunks avec chevauchement"""
    sentences = sent_tokenize(text)
    chunks = []
    current_chunk = []
    current_length = 0

    for sentence in sentences:
        sentence_length = len(sentence)

        if current_length + sentence_length > max_length and current_chunk:
            chunks.append(' '.join(current_chunk))
            # Garder quelques phrases pour le chevauchement
            overlap_sentences = []
            overlap_length = 0
            for sent in reversed(current_chunk):
                overlap_length += len(sent)
                overlap_sentences.insert(0, sent)
                if overlap_length >= overlap:
                    break
            current_chunk = overlap_sentences
            current_length = overlap_length

        current_chunk.append(sentence)
        current_length += sentence_length

    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks

def calculate_keyword_relevance(text, keywords):
    """Calcule un score de pertinence basé sur la présence de mots-clés"""
    text_lower = text.lower()
    keyword_count = 0
    for keyword in keywords:
        if keyword.lower() in text_lower:
            # Donner plus de poids aux mots-clés plus spécifiques
            if len(keyword.split()) > 1:  # Phrases multi-mots
                keyword_count += 2
            else:
                keyword_count += 1

    return keyword_count / (len(keywords) * 1.5) if keywords else 0

def extract_answer_from_chunk(question, context, max_length=512):
    """Utilise le modèle Q&A pour extraire une réponse du contexte"""
    try:
        # Tronquer le contexte si nécessaire
        if len(context) > max_length * 3:
            context = context[:max_length * 3]

        result = qa_pipeline(question=question, context=context)

        # Filtrer les réponses de faible confiance
        if result['score'] > 0.1:  # Seuil de confiance
            return result['answer']
        return None
    except Exception as e:
        print(f"Erreur dans Q&A: {e}")
        return None

def extract_specific_reasoning(chunks, question, keywords):
    """Extrait le raisonnement spécifique basé sur la question et les mots-clés"""
    relevant_passages = []

    for i, chunk in enumerate(chunks):
        # Score de pertinence basé sur les mots-clés
        relevance_score = calculate_keyword_relevance(chunk, keywords)

        # Si le chunk est pertinent
        if relevance_score > 0.15:  # Seuil ajustable
            # Extraire la réponse à la question spécifique
            answer = extract_answer_from_chunk(question, chunk)

            # Si pas de réponse Q&A, extraire les phrases pertinentes
            if not answer:
                sentences = sent_tokenize(chunk)
                relevant_sentences = []
                for sent in sentences:
                    if any(kw.lower() in sent.lower() for kw in keywords[:5]):  # Top 5 keywords
                        relevant_sentences.append(sent)

                if relevant_sentences:
                    answer = ' '.join(relevant_sentences[:3])  # Max 3 phrases

            if answer:
                relevant_passages.append({
                    'content': answer,
                    'relevance': relevance_score,
                    'chunk_index': i,
                    'full_context': chunk
                })

    # Consolider les passages pertinents
    if relevant_passages:
        return consolidate_passages(relevant_passages)

    return "Analysis not found in the document"

def consolidate_passages(passages):
    """Consolide plusieurs passages en un texte cohérent"""
    if not passages:
        return None

    # Trier par pertinence
    passages.sort(key=lambda x: x['relevance'], reverse=True)

    # Prendre les top passages (max 3)
    top_passages = passages[:3]

    # Si un seul passage très pertinent
    if len(top_passages) == 1 or top_passages[0]['relevance'] > 0.5:
        return expand_context(top_passages[0])

    # Sinon, combiner intelligemment
    combined_text = []
    seen_content = set()

    for passage in top_passages:
        content = passage['content'].strip()
        # Éviter les doublons
        content_key = content[:50].lower()
        if content and content_key not in seen_content:
            seen_content.add(content_key)

            # Étendre le contexte si nécessaire
            extended = expand_context(passage)
            combined_text.append(extended)

    return ' '.join(combined_text)

def expand_context(passage):
    """Étend le contexte d'un passage pour plus de clarté"""
    content = passage['content']
    full_context = passage.get('full_context', '')

    if not full_context:
        return content

    # Trouver la position du contenu dans le contexte complet
    content_start = full_context.lower().find(content[:30].lower())
    if content_start == -1:
        return content

    # Prendre un peu plus de contexte avant et après
    sentences = sent_tokenize(full_context)
    target_sent_idx = None

    for i, sent in enumerate(sentences):
        if content[:30] in sent:
            target_sent_idx = i
            break

    if target_sent_idx is not None:
        start_idx = max(0, target_sent_idx - 1)
        end_idx = min(len(sentences), target_sent_idx + 2)
        expanded = ' '.join(sentences[start_idx:end_idx])
        return expanded

    return content

def extract_trademark_reasoning(text):
    """Fonction principale pour extraire le raisonnement des oppositions de marques"""

    # Diviser en chunks
    chunks = split_into_chunks(text)
    print(f"Document divisé en {len(chunks)} chunks")

    # Templates de questions
    extraction_templates = {
        # PARTIE 1 : COMPARAISON DES MARQUES
        "similarity_comparison": {
            "visual_similarity": "What is the analysis of visual similarity between the marks? What visual elements are compared?",
            "aural_similarity": "What is the analysis of phonetic/aural similarity? How are the marks pronounced and compared?",
            "conceptual_similarity": "What is the conceptual comparison? What meanings or ideas do the marks convey?"
        },

        # PARTIE 2 : DISTINCTIVITÉ
        "distinctiveness_analysis": {
            "inherent_distinctiveness": "What is the assessment of the earlier mark's inherent distinctive character?",
            "enhanced_distinctiveness": "Is there any evidence of enhanced distinctiveness through use?",
            "distinctiveness_conclusion": "What is the conclusion on the distinctive character (low, medium, high)?"
        },

        # PARTIE 3 : RISQUE DE CONFUSION
        "confusion_assessment": {
            "direct_confusion": "What is the analysis regarding direct confusion (mistaking one mark for the other)?",
            "indirect_confusion": "What is the analysis regarding indirect confusion (economic link)?",
            "global_assessment": "What factors are considered in the global assessment of confusion?",
            "interdependency": "How is the interdependency principle applied (similarity of marks vs goods)?",
            "final_conclusion": "What is the final conclusion on likelihood of confusion and why?"
        }
    }

    reasoning_structure = {
        "similarity_analysis": {},
        "distinctiveness": {},
        "confusion_risk": {}
    }

    # Extraire les informations pour chaque section
    print("\nExtraction du raisonnement...")

    # 1. Analyse des similarités
    print("\n1. Extraction de l'analyse des similarités...")
    for sub_key, question in extraction_templates["similarity_comparison"].items():
        print(f"   - {sub_key}")
        reasoning_structure["similarity_analysis"][sub_key] = extract_specific_reasoning(
            chunks, question, similarity_keywords[sub_key]
        )

    # 2. Analyse de la distinctivité
    print("\n2. Extraction de l'analyse de distinctivité...")
    for sub_key, question in extraction_templates["distinctiveness_analysis"].items():
        print(f"   - {sub_key}")
        reasoning_structure["distinctiveness"][sub_key] = extract_specific_reasoning(
            chunks, question, distinctiveness_keywords[sub_key]
        )

    # 3. Analyse du risque de confusion
    print("\n3. Extraction de l'analyse du risque de confusion...")
    for sub_key, question in extraction_templates["confusion_assessment"].items():
        print(f"   - {sub_key}")
        reasoning_structure["confusion_risk"][sub_key] = extract_specific_reasoning(
            chunks, question, confusion_keywords[sub_key]
        )

    return reasoning_structure

def generate_trademark_reasoning_summary(reasoning_structure):
    """Génère un résumé structuré du raisonnement"""

    template = """
TRADEMARK OPPOSITION REASONING SUMMARY

==== 1. COMPARISON OF THE MARKS ====

Visual Similarity:
{visual_analysis}

Aural/Phonetic Similarity:
{aural_analysis}

Conceptual Similarity:
{conceptual_analysis}

==== 2. DISTINCTIVE CHARACTER OF THE EARLIER MARK ====

Inherent Distinctiveness:
{inherent_distinct}

Enhanced Distinctiveness:
{enhanced_distinct}

Conclusion on Distinctiveness:
{distinct_conclusion}

==== 3. LIKELIHOOD OF CONFUSION ====

Direct Confusion Analysis:
{direct_conf}

Indirect Confusion Analysis:
{indirect_conf}

Global Assessment:
{global_assess}

Application of Interdependency Principle:
{interdep}

Final Conclusion:
{final_concl}
"""

    # Remplir le template avec les données extraites
    summary_data = {
        'visual_analysis': reasoning_structure['similarity_analysis'].get('visual_similarity', 'Not found'),
        'aural_analysis': reasoning_structure['similarity_analysis'].get('aural_similarity', 'Not found'),
        'conceptual_analysis': reasoning_structure['similarity_analysis'].get('conceptual_similarity', 'Not found'),
        'inherent_distinct': reasoning_structure['distinctiveness'].get('inherent_distinctiveness', 'Not found'),
        'enhanced_distinct': reasoning_structure['distinctiveness'].get('enhanced_distinctiveness', 'Not analyzed'),
        'distinct_conclusion': reasoning_structure['distinctiveness'].get('distinctiveness_conclusion', 'Not found'),
        'direct_conf': reasoning_structure['confusion_risk'].get('direct_confusion', 'Not found'),
        'indirect_conf': reasoning_structure['confusion_risk'].get('indirect_confusion', 'Not analyzed'),
        'global_assess': reasoning_structure['confusion_risk'].get('global_assessment', 'Not found'),
        'interdep': reasoning_structure['confusion_risk'].get('interdependency', 'Not found'),
        'final_concl': reasoning_structure['confusion_risk'].get('final_conclusion', 'Not found')
    }

    return template.format(**summary_data)

# Fonction principale pour traiter un PDF
def process_trademark_opposition_pdf(pdf_path):
    """Traite un PDF d'opposition de marques et extrait le raisonnement"""

    print(f"Traitement du fichier : {pdf_path}")
    print("="*60)

    # 1. Extraction du texte
    print("\n1. Extraction du texte du PDF...")
    text = pdf_to_text(pdf_path)
    print(f"   ✓ Texte extrait : {len(text)} caractères")

    # 2. Extraction du raisonnement
    print("\n2. Analyse du raisonnement juridique...")
    reasoning = extract_trademark_reasoning(text)

    # 3. Génération du résumé
    print("\n3. Génération du résumé structuré...")
    summary = generate_trademark_reasoning_summary(reasoning)

    # 4. Sauvegarde des résultats
    output_data = {
        "file": pdf_path,
        "detailed_reasoning": reasoning,
        "summary": summary
    }

    output_filename = "trademark_reasoning_analysis.json"
    with open(output_filename, "w", encoding="utf-8") as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

    print(f"\n✓ Analyse sauvegardée dans '{output_filename}'")

    # Afficher le résumé
    print("\n" + "="*60)
    print("RÉSUMÉ DU RAISONNEMENT:")
    print("="*60)
    print(summary)

    return output_data

# Interface d'utilisation
if __name__ == "__main__":
    from google.colab import files

    # Upload du fichier PDF
    print("\nVeuillez uploader le fichier PDF d'opposition de marques...")
    uploaded = files.upload()

    if uploaded:
        # Traiter le fichier uploadé
        pdf_path = list(uploaded.keys())[0]

        # Analyser le document
        results = process_trademark_opposition_pdf(pdf_path)

        # Télécharger les résultats
        print("\n✓ Téléchargement du fichier de résultats...")
        files.download("trademark_reasoning_analysis.json")
    else:
        print("Aucun fichier uploadé.")

